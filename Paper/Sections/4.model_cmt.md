**4, Model**

**4,1 Model cho xử lý comment**

Mục tiêu của phần này là mô tả chi tiết kiến trúc, thành phần và các thiết lập quan trọng trong quá trình finetune mô hình PhoBERT-base để phân loại độ tin cậy của review thành ba mức: low, mid và high.

**4.1.1 Kiến trúc PhoBERT và lớp phân loại**\
PhoBERT-base là một biến thể của RoBERTa được huấn luyện trước trên tập văn bản tiếng Việt với dung lượng lên đến 45 GB. Kiến trúc cơ bản bao gồm:

- **12 Transformer encoder layers**: Mỗi layer gồm hai sublayer chính là MultiHead SelfAttention và FeedForward Network, được kết hợp với cơ chế residual connection và layer normalization.
- **Hidden size = 768**: Độ dài vector embedding của mỗi token.
- **12 Attention heads**: Cho phép mô hình học được đa dạng khía cạnh tương quan giữa các token trong câu.

Trên đầu PhoBERT, chúng tôi thêm một tầng phân loại bao gồm:

1. **Pooler g(·)**: Lấy embedding của token [CLS] (hoặc trung bình embedding của tất cả token) làm đại diện cho toàn bộ review. Giả sử đầu ra của encoder với [CLS] là hCLS ∈ R768.
1. **Fullyconnected layer h(·)**: Một lớp linear mapping từ không gian 768 chiều sang không gian 3 chiều (tương ứng 3 lớp). Công thức:

logits = WhCLS + b,	   W ∈ R3×768,  b ∈R3. 

1. **Softmax**: Chuyển logits thành xác suất đầu ra cho mỗi lớp:

pc=exp⁡(logitsc)k=13exp⁡(logitsk).

**4.1.2 Tiền xử lý, tokenization và xây dựng đầu vào**\
Quy trình chuyển văn bản thô thành đầu vào cho model gồm:

- **Tách từ & mã hóa**: Sử dụng AutoTokenizer.from\_pretrained("vinai/phobert-base") với cấu hình:
  - max\_length = 128: Giới hạn độ dài tối đa để cân bằng giữa thông tin và chi phí tính toán.
  - padding = 'max\_length': Thêm token [PAD] cho những chuỗi ngắn hơn.
  - truncation = True: Cắt bớt chuỗi dài vượt 128 tokens.
- **Tạo attention mask**: Mỗi vị trí padding được đánh dấu mask = 0, token thực mask = 1, để model biết bỏ qua phần đệm.
- **Gán nhãn**: Chuyển Helpfulness score gốc (1–5) thành 3 lớp số nguyên {0,1,2}:
  - 0 (low): score ≤ 2
  - 1 (mid): score = 3
  - 2 (high): score ≥ 4
- **Bộ dữ liệu dưới dạng PyTorch tensors**: Các trường input\_ids, attention\_mask, và labels được đóng gói vào Dataset của Hugging Face hoặc TensorDataset của PyTorch để phục vụ Trainer hoặc DataLoader.

**4.1.3 Hàm mất mát và chiến lược tối ưu**

- **Crossentropy loss** cho bài toán phân loại đa lớp:

LcE=-1Bi=1Bc=13yi,clogpi,c.

trong đó yi,c​ là onehot nhãn thực và pi,c​ là xác suất dự đoán lớp c.

- **Optimizer**: AdamW với tham số:
  - Learning rate = 2×10-5
  - Weight decay = 0.01
- **Scheduler**: Linear decay LR sau khi warmup 10% tổng số bước training. Warmup giúp ổn định gradient ban đầu và tránh “cold start”.
- **Regularization & Early stopping**: Theo dõi metric macroF1 trên tập validation; dừng trước nếu không cải thiện trong 2 epoch liên tiếp.

**4.1.4 Thiết lập huấn luyện và đánh giá**

- **Batch size**: 16 mẫu cho huấn luyện, 32 mẫu cho đánh giá.
- **Epochs**: Tối đa 5 epoch, thường dừng sớm ở 3–4 epoch khi convergence.
- **Chia dữ liệu**: 80% train, 20% validation, stratified sampling để giữ phân bố nhãn đồng đều.
- **Metrics**:
  - **Accuracy**: Đơn giản, đo tỷ lệ dự đoán đúng.
  - **MacroF1**: Trung bình F1 của mỗi lớp, phản ánh công bằng với những lớp ít mẫu hoặc bị lệch.

**4.1.5 Quy trình PyTorch thuần** \
Ngoài sử dụng Trainer của Hugging Face, chúng tôi cũng xây dựng vòng lặp huấn luyện thủ công để kiểm soát chi tiết:

1. **DataLoader**: Tạo từ TensorDataset(input\_ids, attention\_mask, labels) với shuffle cho train, không shuffle cho validation.
1. **Vòng train\_epoch()**:
   1. Chuyển model.train(), tính loss, backpropagate, optimizer.step(), scheduler.step(), optimizer.zero\_grad().
1. **Vòng eval\_epoch()**:
   1. Chuyển model.eval(), tắt gradient với torch.no\_grad(), thu prediction, tính Accuracy và MacroF1.
1. **Logging & checkpoint**: Ghi lại loss và metrics, lưu model tốt nhất.

